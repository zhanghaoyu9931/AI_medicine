{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Data Analysis with Deep Learning\n",
    "\n",
    "This notebook demonstrates the use of deep learning for medical data analysis, supporting both **medical images** and **ECG signals** with classification and regression tasks. \n",
    "\n",
    "## Pipeline Overview:\n",
    "1. **Data preprocessing** (image enhancement or ECG signal processing)\n",
    "2. **Dataset creation and data loading** (with proper train/val/test splits)\n",
    "3. **Model architecture definition** (2D CNN for images, 1D CNN for ECG)\n",
    "4. **Hyperparameter tuning** (grid search optimization)\n",
    "5. **Model training and evaluation** (with early stopping and metrics)\n",
    "6. **Results visualization** (training curves, confusion matrices, etc.)\n",
    "\n",
    "## Key Features:\n",
    "- **Unified interface** for both image and ECG data\n",
    "- **Modular design** with separate preprocessing, dataset, and model modules\n",
    "- **Robust ECG processing** including PhysioNet format support\n",
    "- **Comprehensive evaluation** with classification and regression metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Import image processing functions\n",
    "from preprocessing import (\n",
    "    process_single_image, visualize_preprocessing, process_all_images,\n",
    "    # ECG processing functions\n",
    "    read_physionet_data, process_ecg_signal, process_all_ecg_signals, preprocess_jiachen_files,\n",
    "    visualize_ecg_preprocessing,\n",
    "    # voice processing functions\n",
    "    read_voice_data, read_voice_metadata, preprocess_voice_signal,\n",
    "    process_voice_file, process_all_voice_signals, create_voice_labels_file,\n",
    "    visualize_voice_preprocessing\n",
    ")\n",
    "\n",
    "# Import dataset functions\n",
    "from dataset import (\n",
    "    MedicalImageDataset, ECGDataset, \n",
    "    create_data_loaders, create_ecg_data_loaders, create_voice_data_loaders,\n",
    "    create_asd_data_loaders, ASDDataset  # Added ASD dataset support\n",
    ")\n",
    "\n",
    "# Import model classes\n",
    "from model import MedicalCNN, ECG1DCNN, ModelTrainer, Voice1DCNN, ASDTabularModel  # Added ASD model\n",
    "\n",
    "# Import hyperparameter tuning\n",
    "from hyperparameter_tuning import HyperparameterTuner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Set up the configuration for the experiment. You can modify these parameters to experiment with different settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_name = '' # don't change this\n",
    "\n",
    "# Experiment configuration\n",
    "config = {\n",
    "    'data_type': 'tabular',  # 'image', 'voice' 'ECG' or 'tabular' - determines data processing pipeline\n",
    "    'task_type': 'classification',  # 'classification' or 'regression'\n",
    "    'num_classes': 2,  # for classification only\n",
    "    'image_dir': f'./data/{student_name}after_processed',\n",
    "    'labels_file': f'./data/{student_name}labels.csv',\n",
    "    'batch_size': 32,\n",
    "    'num_epochs': 10,\n",
    "    'learning_rate': 0.001,\n",
    "    'train_ratio': 0.7,\n",
    "    'val_ratio': 0.15,\n",
    "    'test_ratio': 0.15,\n",
    "    'random_seed': 42,\n",
    "    'early_stopping_patience': 15,\n",
    "    'save_dir': './results',\n",
    "    \n",
    "    # ECG-specific parameters\n",
    "    'ecg_max_length': 500,  # Target sequence length for ECG signals (uniform sampling)\n",
    "    \n",
    "    # voice-specific parameters\n",
    "    'voice_max_length': 500,  # 5 seconds at 8kHz\n",
    "    'target_variable': 'Voice Handicap Index (VHI) Score',  # 'VHI Score', 'RSI Score', 'Diagnosis'\n",
    "    \n",
    "    # Grid search parameters\n",
    "    'grid_search': {\n",
    "        'num_conv_layers': [3],\n",
    "        'conv_channels': [64],\n",
    "        'fc_layers': [[128, 64], [128, 32]],  # , [1024, 256, 64]\n",
    "        'learning_rate': [0.01] # , 0.0001\n",
    "    }\n",
    "}\n",
    "\n",
    "# directly set the class names according to your data\n",
    "class_nms = {\n",
    "    0: 'Normal',\n",
    "    1: 'Abnormal'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Let's preprocess the data based on the data type (image or ECG signals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabular data preprocessing:\n",
      "- No preprocessing required for tabular data\n",
      "- Data is loaded directly from CSV file\n",
      "- ASD labels file: ./data/Jingyi/labels.csv\n",
      "- Dataset shape: (800, 20)\n",
      "- Features: ['ID', 'A1_Score', 'A2_Score', 'A3_Score', 'A4_Score', 'A5_Score', 'A6_Score', 'A7_Score', 'A8_Score', 'A9_Score', 'A10_Score', 'age', 'gender', 'ethnicity', 'jaundice', 'austim', 'contry_of_res', 'result', 'age_desc', 'y']\n",
      "- Target distribution: {0: 639, 1: 161}\n",
      "- Sample data:\n",
      "   ID  A1_Score  A2_Score  A3_Score  A4_Score  A5_Score  A6_Score  A7_Score  \\\n",
      "0   1         1         0         1         0         1         0         1   \n",
      "1   2         0         0         0         0         0         0         0   \n",
      "2   3         1         1         1         1         1         1         1   \n",
      "3   4         0         0         0         0         0         0         0   \n",
      "4   5         0         0         0         0         0         0         0   \n",
      "\n",
      "   A8_Score  A9_Score  A10_Score        age gender       ethnicity jaundice  \\\n",
      "0         0         1          1  38.172746      f               ?       no   \n",
      "1         0         0          0  47.750517      m               ?       no   \n",
      "2         1         1          1   7.380373      m  White-European       no   \n",
      "3         0         0          0  23.561927      f               ?       no   \n",
      "4         0         0          0  43.205790      m               ?       no   \n",
      "\n",
      "  austim  contry_of_res     result     age_desc  y  \n",
      "0     no        Austria   6.351166  18 and more  0  \n",
      "1     no          India   2.255185  18 and more  0  \n",
      "2    yes  United States  14.851484  18 and more  1  \n",
      "3     no  United States   2.276617  18 and more  0  \n",
      "4     no   South Africa  -4.777286  18 and more  0  \n"
     ]
    }
   ],
   "source": [
    "if config['data_type'] == 'image':\n",
    "    # Visualize preprocessing on a sample image\n",
    "    ori_image_dir = f'./data/{student_name}ori_images'\n",
    "    sample_image_path = list(Path(ori_image_dir).glob('*.png'))[0]\n",
    "    visualize_preprocessing(sample_image_path)\n",
    "    \n",
    "elif config['data_type'] == 'ECG':\n",
    "    label_df = pd.read_csv(config['labels_file'])\n",
    "    label_df.columns = [x.strip() for x in label_df.columns]\n",
    "    label_df['y'] = label_df['cause of death'].astype(int)\n",
    "    # label_df['id'] = label_df['id'].apply(lambda x: x.replace('Jiachen_', 'P'))\n",
    "    label_df.to_csv(config['labels_file'], index=False)\n",
    "    \n",
    "    # ECG data directories\n",
    "    ecg_raw_dir = f'./data/{student_name}ori_images'\n",
    "    ecg_processed_dir = f'./data/{student_name}after_processed'\n",
    "    \n",
    "    # Handle Jiachen's special file naming\n",
    "    preprocess_jiachen_files(ecg_raw_dir, config['labels_file'])\n",
    "    \n",
    "    # Process all ECG files\n",
    "    processed_count = process_all_ecg_signals(\n",
    "        ecg_raw_dir, \n",
    "        ecg_processed_dir, \n",
    "        config['ecg_max_length']\n",
    "    )\n",
    "    \n",
    "    # Visualize preprocessing on a sample ECG\n",
    "    sample_files = list(Path(ecg_raw_dir).glob('*.dat'))\n",
    "    if sample_files:\n",
    "        sample_dat = sample_files[0]\n",
    "        sample_hea = Path(str(sample_dat).replace('.dat', '.hea'))\n",
    "        if sample_hea.exists():\n",
    "            visualize_ecg_preprocessing(sample_dat, sample_hea, config['ecg_max_length'])\n",
    "        else:\n",
    "            print(f\"Warning: Header file {sample_hea} not found\")\n",
    "    else:\n",
    "        print(\"No ECG files found for visualization\")\n",
    "\n",
    "elif config['data_type'] == 'voice':\n",
    "    # voice data directories\n",
    "    voice_raw_dir = f'./data/{student_name}ori_images'\n",
    "    voice_processed_dir = f'./data/{student_name}after_processed'\n",
    "    \n",
    "    # Process all voice files\n",
    "    processed_count = process_all_voice_signals(\n",
    "        voice_raw_dir, \n",
    "        voice_processed_dir, \n",
    "        config['voice_max_length']\n",
    "    )\n",
    "\n",
    "    # Create labels file\n",
    "    labels_df = create_voice_labels_file(\n",
    "        voice_raw_dir, \n",
    "        config['labels_file'], \n",
    "        config['target_variable']\n",
    "    )\n",
    "    print(labels_df.head())\n",
    "    labels_df.to_csv(config['labels_file'], index=False)\n",
    "\n",
    "    # Visualize preprocessing on a sample voice signal\n",
    "    sample_files = list(Path(voice_raw_dir).glob('*.dat'))\n",
    "    if sample_files:\n",
    "        sample_dat = sample_files[0]\n",
    "        sample_hea = Path(str(sample_dat).replace('.dat', '.hea'))\n",
    "        sample_txt = Path(str(sample_dat).replace('.dat', '.txt'))\n",
    "        if sample_hea.exists() and sample_txt.exists():\n",
    "            visualize_voice_preprocessing(sample_txt, sample_hea, config['voice_max_length'], 3)\n",
    "        else:\n",
    "            print(f\"Warning: Header file {sample_hea} not found\")\n",
    "    else:\n",
    "        print(\"No voice files found for visualization\")\n",
    "\n",
    "elif config['data_type'] == 'tabular':\n",
    "    # For tabular data (ASD), no preprocessing is needed - data is already in CSV format\n",
    "    print(\"Tabular data preprocessing:\")\n",
    "    print(\"- No preprocessing required for tabular data\")\n",
    "    print(\"- Data is loaded directly from CSV file\")\n",
    "    print(f\"- ASD labels file: {config['labels_file']}\")\n",
    "    \n",
    "    # Check if ASD labels file exists\n",
    "    asd_file_path = Path(config['labels_file'])\n",
    "    if asd_file_path.exists():\n",
    "        # Load and show basic info about the ASD dataset\n",
    "        asd_data = pd.read_csv(config['labels_file'])\n",
    "        print(f\"- Dataset shape: {asd_data.shape}\")\n",
    "        print(f\"- Features: {list(asd_data.columns)}\")\n",
    "        print(f\"- Target distribution: {asd_data['y'].value_counts().to_dict()}\")\n",
    "        print(f\"- Sample data:\")\n",
    "        print(asd_data.head())\n",
    "    else:\n",
    "        print(f\"Warning: ASD labels file not found at {config['labels_file']}\")\n",
    "        \n",
    "else:\n",
    "    print(f\"Unknown data type: {config['data_type']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's process all images in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['data_type'] == 'image':\n",
    "    # Process all images\n",
    "    input_dir = ori_image_dir\n",
    "    output_dir = config['image_dir']\n",
    "    target_size = (224, 224)  # Standard size for many CNN architectures\n",
    "\n",
    "    process_all_images(input_dir, output_dir, target_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Create data loaders for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating ASD tabular data loaders\n",
      "Feature dimension: 18\n",
      "Training set size: 560\n",
      "Validation set size: 120\n",
      "Test set size: 120\n",
      "Data type: tabular\n",
      "Sample data shape: torch.Size([32, 18])\n",
      "Sample label shape: torch.Size([32])\n",
      "Sample features (first 5): tensor([ 0.8980,  0.9377, -0.8980, -0.8597, -0.8256])\n",
      "Sample labels (first 5): tensor([0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders based on data type\n",
    "if config['data_type'] == 'image':\n",
    "    data_loaders = create_data_loaders(\n",
    "        data_dir=config['image_dir'],\n",
    "        labels_file=config['labels_file'],\n",
    "        task_type=config['task_type'],\n",
    "        batch_size=config['batch_size'],\n",
    "        train_ratio=config['train_ratio'],\n",
    "        val_ratio=config['val_ratio'],\n",
    "        test_ratio=config['test_ratio'],\n",
    "        random_seed=config['random_seed']\n",
    "    )\n",
    "elif config['data_type'] == 'ECG':\n",
    "    # For ECG data, use processed ECG data directory\n",
    "    data_loaders = create_ecg_data_loaders(\n",
    "        data_dir=config['image_dir'],\n",
    "        labels_file=config['labels_file'],\n",
    "        task_type=config['task_type'],\n",
    "        batch_size=config['batch_size'],\n",
    "        train_ratio=config['train_ratio'],\n",
    "        val_ratio=config['val_ratio'],\n",
    "        test_ratio=config['test_ratio'],\n",
    "        random_seed=config['random_seed']\n",
    "    )\n",
    "elif config['data_type'] == 'voice':\n",
    "    print('Creating voice data loaders')\n",
    "    data_loaders = create_voice_data_loaders(\n",
    "        data_dir=config['image_dir'],\n",
    "        labels_file=config['labels_file'],\n",
    "        task_type=config['task_type'],\n",
    "        batch_size=config['batch_size'],\n",
    "        train_ratio=config['train_ratio'],\n",
    "        val_ratio=config['val_ratio'],\n",
    "        test_ratio=config['test_ratio'],\n",
    "        random_seed=config['random_seed'],\n",
    "        target_length=config['voice_max_length']\n",
    "    )\n",
    "elif config['data_type'] == 'tabular':\n",
    "    print('Creating ASD tabular data loaders')\n",
    "    data_loaders_info = create_asd_data_loaders(\n",
    "        labels_file=config['labels_file'],\n",
    "        task_type=config['task_type'],\n",
    "        batch_size=config['batch_size'],\n",
    "        train_ratio=config['train_ratio'],\n",
    "        val_ratio=config['val_ratio'],\n",
    "        test_ratio=config['test_ratio'],\n",
    "        random_seed=config['random_seed']\n",
    "    )\n",
    "    # Extract data loaders and feature dimension for tabular data\n",
    "    data_loaders = {\n",
    "        'train': data_loaders_info['train'],\n",
    "        'val': data_loaders_info['val'],\n",
    "        'test': data_loaders_info['test']\n",
    "    }\n",
    "    feature_dim = data_loaders_info['feature_dim']\n",
    "    print(f\"Feature dimension: {feature_dim}\")\n",
    "\n",
    "train_loader, val_loader, test_loader = data_loaders['train'], data_loaders['val'], data_loaders['test']\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Training set size: {len(data_loaders['train'].dataset)}\")\n",
    "print(f\"Validation set size: {len(data_loaders['val'].dataset)}\")\n",
    "print(f\"Test set size: {len(data_loaders['test'].dataset)}\")\n",
    "print(f\"Data type: {config['data_type']}\")\n",
    "\n",
    "# Show sample data shape\n",
    "sample_data, sample_label = next(iter(train_loader))\n",
    "print(f\"Sample data shape: {sample_data.shape}\")\n",
    "print(f\"Sample label shape: {sample_label.shape}\")\n",
    "if config['data_type'] == 'tabular':\n",
    "    print(f\"Sample features (first 5): {sample_data[0, :5]}\")\n",
    "    print(f\"Sample labels (first 5): {sample_label[:5]}\")\n",
    "else:\n",
    "    sample_data[:3,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "Perform grid search to find the best model architecture and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Performing tabular grid search...\n",
      "\n",
      "Trying combination 1/2:\n",
      "{\n",
      "  \"hidden_layers\": [\n",
      "    128,\n",
      "    64\n",
      "  ],\n",
      "  \"learning_rate\": 0.01\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 1/10 [00:44<06:44, 44.99s/it, Train Loss=0.5184, Val Loss=0.2637, Train Acc=0.784, Val Acc=0.858, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/10]\n",
      "Train Loss: 0.5184, Train Metrics: {'accuracy': 0.7839285714285714, 'precision': 0.8062122070243365, 'recall': 0.7839285714285714, 'f1': 0.7929139882475614, 'aupr': 0.3500303653181351, 'auc': 0.7075757575757575}\n",
      "Val Loss: 0.2637, Val Metrics: {'accuracy': 0.8583333333333333, 'precision': 0.8953125, 'recall': 0.8583333333333333, 'f1': 0.868867924528302, 'aupr': 0.5071428571428571, 'auc': 0.8578643578643578}\n",
      "New best model saved with validation loss: 0.2637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 2/10 [01:29<05:57, 44.67s/it, Train Loss=0.3805, Val Loss=0.2366, Train Acc=0.846, Val Acc=0.867, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/10]\n",
      "Train Loss: 0.3805, Train Metrics: {'accuracy': 0.8464285714285714, 'precision': 0.8381211180124223, 'recall': 0.8464285714285714, 'f1': 0.8410918822974734, 'aupr': 0.4252682100508187, 'auc': 0.7258585858585859}\n",
      "Val Loss: 0.2366, Val Metrics: {'accuracy': 0.8666666666666667, 'precision': 0.8778947368421052, 'recall': 0.8666666666666667, 'f1': 0.8710891976692067, 'aupr': 0.47857142857142854, 'auc': 0.8066378066378067}\n",
      "New best model saved with validation loss: 0.2366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 3/10 [02:13<05:11, 44.55s/it, Train Loss=0.3229, Val Loss=0.2356, Train Acc=0.834, Val Acc=0.908, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/10]\n",
      "Train Loss: 0.3229, Train Metrics: {'accuracy': 0.8339285714285715, 'precision': 0.8232982486632382, 'recall': 0.8339285714285715, 'f1': 0.8270593310900711, 'aupr': 0.3887111484021596, 'auc': 0.7009090909090909}\n",
      "Val Loss: 0.2356, Val Metrics: {'accuracy': 0.9083333333333333, 'precision': 0.9258152173913043, 'recall': 0.9083333333333333, 'f1': 0.9132011967090501, 'aupr': 0.6306122448979592, 'auc': 0.9069264069264068}\n",
      "New best model saved with validation loss: 0.2356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 4/10 [02:58<04:27, 44.50s/it, Train Loss=0.3306, Val Loss=0.2347, Train Acc=0.832, Val Acc=0.892, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/10]\n",
      "Train Loss: 0.3306, Train Metrics: {'accuracy': 0.8321428571428572, 'precision': 0.8266614906832298, 'recall': 0.8321428571428572, 'f1': 0.8290685504971219, 'aupr': 0.3986753246753246, 'auc': 0.716969696969697}\n",
      "Val Loss: 0.2347, Val Metrics: {'accuracy': 0.8916666666666667, 'precision': 0.8986979166666667, 'recall': 0.8916666666666667, 'f1': 0.8944444444444445, 'aupr': 0.5496031746031745, 'auc': 0.8405483405483406}\n",
      "New best model saved with validation loss: 0.2347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  50%|█████     | 5/10 [03:43<03:43, 44.65s/it, Train Loss=0.3081, Val Loss=0.2409, Train Acc=0.848, Val Acc=0.850, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/10]\n",
      "Train Loss: 0.3081, Train Metrics: {'accuracy': 0.8482142857142857, 'precision': 0.8389508848698, 'recall': 0.8482142857142857, 'f1': 0.841935947770495, 'aupr': 0.4265121114840216, 'auc': 0.7235353535353535}\n",
      "Val Loss: 0.2409, Val Metrics: {'accuracy': 0.85, 'precision': 0.8762220538082608, 'recall': 0.85, 'f1': 0.8588421052631579, 'aupr': 0.4620279146141215, 'auc': 0.8152958152958154}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 6/10 [04:28<02:59, 44.99s/it, Train Loss=0.2807, Val Loss=0.2448, Train Acc=0.854, Val Acc=0.867, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/10]\n",
      "Train Loss: 0.2807, Train Metrics: {'accuracy': 0.8535714285714285, 'precision': 0.8497929739581422, 'recall': 0.8535714285714285, 'f1': 0.851453685122956, 'aupr': 0.45691685765215173, 'auc': 0.7543434343434343}\n",
      "Val Loss: 0.2448, Val Metrics: {'accuracy': 0.8666666666666667, 'precision': 0.8778947368421052, 'recall': 0.8666666666666667, 'f1': 0.8710891976692067, 'aupr': 0.47857142857142854, 'auc': 0.8066378066378067}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  70%|███████   | 7/10 [05:13<02:14, 44.94s/it, Train Loss=0.3205, Val Loss=0.2424, Train Acc=0.841, Val Acc=0.875, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/10]\n",
      "Train Loss: 0.3205, Train Metrics: {'accuracy': 0.8410714285714286, 'precision': 0.8462313496827303, 'recall': 0.8410714285714286, 'f1': 0.8433918095490148, 'aupr': 0.4457601222307105, 'auc': 0.7637373737373736}\n",
      "Val Loss: 0.2424, Val Metrics: {'accuracy': 0.875, 'precision': 0.877435064935065, 'recall': 0.875, 'f1': 0.8761362294888443, 'aupr': 0.48257575757575755, 'auc': 0.7929292929292929}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 8/10 [05:58<01:29, 44.81s/it, Train Loss=0.3044, Val Loss=0.2388, Train Acc=0.841, Val Acc=0.858, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/10]\n",
      "Train Loss: 0.3044, Train Metrics: {'accuracy': 0.8410714285714286, 'precision': 0.828270150556163, 'recall': 0.8410714285714286, 'f1': 0.8314867564144781, 'aupr': 0.3972763347763348, 'auc': 0.6984848484848485}\n",
      "Val Loss: 0.2388, Val Metrics: {'accuracy': 0.8583333333333333, 'precision': 0.8733019639934534, 'recall': 0.8583333333333333, 'f1': 0.8640337338771912, 'aupr': 0.46208791208791206, 'auc': 0.8015873015873016}\n",
      "Epoch 00008: reducing learning rate of group 0 to 5.0000e-03.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  90%|█████████ | 9/10 [06:43<00:44, 44.85s/it, Train Loss=0.3034, Val Loss=0.2460, Train Acc=0.843, Val Acc=0.867, LR=0.005000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/10]\n",
      "Train Loss: 0.3034, Train Metrics: {'accuracy': 0.8428571428571429, 'precision': 0.8377950310559005, 'recall': 0.8428571428571429, 'f1': 0.8399790685504972, 'aupr': 0.4257727272727273, 'auc': 0.733939393939394}\n",
      "Val Loss: 0.2460, Val Metrics: {'accuracy': 0.8666666666666667, 'precision': 0.8988039144617617, 'recall': 0.8666666666666667, 'f1': 0.8759410801963993, 'aupr': 0.5226958525345622, 'auc': 0.862914862914863}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [07:27<00:00, 44.76s/it, Train Loss=0.2914, Val Loss=0.2364, Train Acc=0.846, Val Acc=0.867, LR=0.005000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/10]\n",
      "Train Loss: 0.2914, Train Metrics: {'accuracy': 0.8464285714285714, 'precision': 0.8453873437280517, 'recall': 0.8464285714285714, 'f1': 0.8458943719069028, 'aupr': 0.44523809523809527, 'auc': 0.7533333333333334}\n",
      "Val Loss: 0.2364, Val Metrics: {'accuracy': 0.8666666666666667, 'precision': 0.871985656656208, 'recall': 0.8666666666666667, 'f1': 0.8690166975881262, 'aupr': 0.46413043478260874, 'auc': 0.7878787878787877}\n",
      "\n",
      "Trying combination 2/2:\n",
      "{\n",
      "  \"hidden_layers\": [\n",
      "    128,\n",
      "    32\n",
      "  ],\n",
      "  \"learning_rate\": 0.01\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 1/10 [00:44<06:43, 44.89s/it, Train Loss=0.5080, Val Loss=0.2540, Train Acc=0.759, Val Acc=0.867, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [1/10]\n",
      "Train Loss: 0.5080, Train Metrics: {'accuracy': 0.7589285714285714, 'precision': 0.7746147155781321, 'recall': 0.7589285714285714, 'f1': 0.7659113869000409, 'aupr': 0.29412815866304237, 'auc': 0.6508080808080807}\n",
      "Val Loss: 0.2540, Val Metrics: {'accuracy': 0.8666666666666667, 'precision': 0.8913224706328154, 'recall': 0.8666666666666667, 'f1': 0.8745263157894737, 'aupr': 0.5078817733990147, 'auc': 0.8441558441558441}\n",
      "New best model saved with validation loss: 0.2540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 2/10 [01:29<05:57, 44.65s/it, Train Loss=0.3032, Val Loss=0.2476, Train Acc=0.839, Val Acc=0.850, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [2/10]\n",
      "Train Loss: 0.3032, Train Metrics: {'accuracy': 0.8392857142857143, 'precision': 0.834083850931677, 'recall': 0.8392857142857143, 'f1': 0.8363422291993721, 'aupr': 0.4165584415584415, 'auc': 0.7282828282828283}\n",
      "Val Loss: 0.2476, Val Metrics: {'accuracy': 0.85, 'precision': 0.8920062695924765, 'recall': 0.85, 'f1': 0.8618279569892473, 'aupr': 0.4925324675324675, 'auc': 0.8528138528138529}\n",
      "New best model saved with validation loss: 0.2476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 3/10 [02:14<05:12, 44.68s/it, Train Loss=0.3150, Val Loss=0.2433, Train Acc=0.836, Val Acc=0.867, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [3/10]\n",
      "Train Loss: 0.3150, Train Metrics: {'accuracy': 0.8357142857142857, 'precision': 0.8204498850431865, 'recall': 0.8357142857142857, 'f1': 0.8236891358049055, 'aupr': 0.37673444976076553, 'auc': 0.6814141414141415}\n",
      "Val Loss: 0.2433, Val Metrics: {'accuracy': 0.8666666666666667, 'precision': 0.8988039144617617, 'recall': 0.8666666666666667, 'f1': 0.8759410801963993, 'aupr': 0.5226958525345622, 'auc': 0.862914862914863}\n",
      "New best model saved with validation loss: 0.2433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 4/10 [02:58<04:27, 44.58s/it, Train Loss=0.3189, Val Loss=0.2439, Train Acc=0.857, Val Acc=0.867, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [4/10]\n",
      "Train Loss: 0.3189, Train Metrics: {'accuracy': 0.8571428571428571, 'precision': 0.8552430031941295, 'recall': 0.8571428571428571, 'f1': 0.856136161445896, 'aupr': 0.47156946826758145, 'auc': 0.7668686868686868}\n",
      "Val Loss: 0.2439, Val Metrics: {'accuracy': 0.8666666666666667, 'precision': 0.8988039144617617, 'recall': 0.8666666666666667, 'f1': 0.8759410801963993, 'aupr': 0.5226958525345622, 'auc': 0.862914862914863}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  50%|█████     | 5/10 [03:42<03:42, 44.49s/it, Train Loss=0.3123, Val Loss=0.2449, Train Acc=0.832, Val Acc=0.867, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/10]\n",
      "Train Loss: 0.3123, Train Metrics: {'accuracy': 0.8321428571428572, 'precision': 0.8246356732348111, 'recall': 0.8321428571428572, 'f1': 0.8277245316345825, 'aupr': 0.39339826839826836, 'auc': 0.7101010101010101}\n",
      "Val Loss: 0.2449, Val Metrics: {'accuracy': 0.8666666666666667, 'precision': 0.871985656656208, 'recall': 0.8666666666666667, 'f1': 0.8690166975881262, 'aupr': 0.46413043478260874, 'auc': 0.7878787878787877}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  60%|██████    | 6/10 [04:27<02:57, 44.47s/it, Train Loss=0.3122, Val Loss=0.2404, Train Acc=0.845, Val Acc=0.867, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [6/10]\n",
      "Train Loss: 0.3122, Train Metrics: {'accuracy': 0.8446428571428571, 'precision': 0.8312999112688554, 'recall': 0.8446428571428571, 'f1': 0.8336823475370856, 'aupr': 0.4023022432113341, 'auc': 0.6972727272727273}\n",
      "Val Loss: 0.2404, Val Metrics: {'accuracy': 0.8666666666666667, 'precision': 0.8913224706328154, 'recall': 0.8666666666666667, 'f1': 0.8745263157894737, 'aupr': 0.5078817733990147, 'auc': 0.8441558441558441}\n",
      "New best model saved with validation loss: 0.2404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  70%|███████   | 7/10 [05:12<02:13, 44.58s/it, Train Loss=0.3074, Val Loss=0.2377, Train Acc=0.832, Val Acc=0.883, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [7/10]\n",
      "Train Loss: 0.3074, Train Metrics: {'accuracy': 0.8321428571428572, 'precision': 0.8246356732348111, 'recall': 0.8321428571428572, 'f1': 0.8277245316345825, 'aupr': 0.39339826839826836, 'auc': 0.7101010101010101}\n",
      "Val Loss: 0.2377, Val Metrics: {'accuracy': 0.8833333333333333, 'precision': 0.9064228874573703, 'recall': 0.8833333333333333, 'f1': 0.8902105263157895, 'aupr': 0.5570197044334976, 'auc': 0.873015873015873}\n",
      "New best model saved with validation loss: 0.2377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  80%|████████  | 8/10 [05:56<01:29, 44.58s/it, Train Loss=0.2888, Val Loss=0.2405, Train Acc=0.832, Val Acc=0.867, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [8/10]\n",
      "Train Loss: 0.2888, Train Metrics: {'accuracy': 0.8321428571428572, 'precision': 0.8217916244511989, 'recall': 0.8321428571428572, 'f1': 0.8255745341614907, 'aupr': 0.385523088023088, 'auc': 0.6997979797979799}\n",
      "Val Loss: 0.2405, Val Metrics: {'accuracy': 0.8666666666666667, 'precision': 0.8666666666666667, 'recall': 0.8666666666666667, 'f1': 0.8666666666666667, 'aupr': 0.44988662131519275, 'auc': 0.7691197691197692}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  90%|█████████ | 9/10 [06:41<00:44, 44.61s/it, Train Loss=0.2856, Val Loss=0.2416, Train Acc=0.834, Val Acc=0.858, LR=0.010000]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [9/10]\n",
      "Train Loss: 0.2856, Train Metrics: {'accuracy': 0.8339285714285715, 'precision': 0.8224418257497649, 'recall': 0.8339285714285715, 'f1': 0.8263031920535316, 'aupr': 0.3861210628452008, 'auc': 0.6974747474747475}\n",
      "Val Loss: 0.2416, Val Metrics: {'accuracy': 0.8583333333333333, 'precision': 0.8610621521335807, 'recall': 0.8583333333333333, 'f1': 0.8596210600873567, 'aupr': 0.4324675324675325, 'auc': 0.764069264069264}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 10/10 [07:26<00:00, 44.60s/it, Train Loss=0.2744, Val Loss=0.2374, Train Acc=0.861, Val Acc=0.875, LR=0.010000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [10/10]\n",
      "Train Loss: 0.2744, Train Metrics: {'accuracy': 0.8607142857142858, 'precision': 0.859773727583462, 'recall': 0.8607142857142858, 'f1': 0.8602297791713769, 'aupr': 0.4838864838864839, 'auc': 0.7759595959595958}\n",
      "Val Loss: 0.2374, Val Metrics: {'accuracy': 0.875, 'precision': 0.877435064935065, 'recall': 0.875, 'f1': 0.8761362294888443, 'aupr': 0.48257575757575755, 'auc': 0.7929292929292929}\n",
      "New best model saved with validation loss: 0.2374\n",
      "\n",
      "Best combination:\n",
      "{\n",
      "  \"hidden_layers\": [\n",
      "    128,\n",
      "    64\n",
      "  ],\n",
      "  \"learning_rate\": 0.01\n",
      "}\n",
      "\n",
      "Best validation metrics:\n",
      "{\n",
      "  \"accuracy\": 0.8916666666666667,\n",
      "  \"precision\": 0.8986979166666667,\n",
      "  \"recall\": 0.8916666666666667,\n",
      "  \"f1\": 0.8944444444444445,\n",
      "  \"aupr\": 0.5496031746031745,\n",
      "  \"auc\": 0.8405483405483406\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Create universal hyperparameter tuner\n",
    "print(f\"Performing {config['data_type']} grid search...\")\n",
    "\n",
    "# Prepare parameters for different data types\n",
    "if config['data_type'] == 'tabular':\n",
    "    # For tabular data, use specific grid search parameters\n",
    "    param_grid = {\n",
    "        'hidden_layers': config['grid_search']['fc_layers'],\n",
    "        'learning_rate': config['grid_search']['learning_rate']\n",
    "    }\n",
    "    \n",
    "    tuner = HyperparameterTuner(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        task_type=config['task_type'],\n",
    "        model_type=config['data_type'],\n",
    "        num_classes=config['num_classes'],\n",
    "        input_dim=feature_dim,  # Pass feature dimension for tabular data\n",
    "        device=device,\n",
    "        save_dir=Path(config['save_dir']) / 'grid_search'\n",
    "    )\n",
    "else:\n",
    "    # For CNN models (image, ECG, voice)\n",
    "    param_grid = {\n",
    "        'num_conv_layers': config['grid_search']['num_conv_layers'],\n",
    "        'conv_channels': config['grid_search']['conv_channels'],\n",
    "        'fc_layers': config['grid_search']['fc_layers'],\n",
    "        'learning_rate': config['grid_search']['learning_rate']\n",
    "    }\n",
    "    \n",
    "    tuner = HyperparameterTuner(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        task_type=config['task_type'],\n",
    "        model_type=config['data_type'],  # 'image' or 'ECG' or 'voice'\n",
    "        num_classes=config['num_classes'],\n",
    "        input_length=config.get('ecg_max_length', 5000) if config['data_type'] == 'ECG' else config.get('voice_max_length', 5000) if config['data_type'] == 'voice' else None, \n",
    "        device=device,\n",
    "        save_dir=Path(config['save_dir']) / 'grid_search'\n",
    "    )\n",
    "\n",
    "# Perform grid search\n",
    "grid_search_results = tuner.grid_search(\n",
    "    param_grid=param_grid,\n",
    "    num_epochs=config['num_epochs'],\n",
    "    early_stopping_patience=config['early_stopping_patience']\n",
    ")\n",
    "\n",
    "# Plot results\n",
    "tuner.plot_results()\n",
    "\n",
    "# Print best combination\n",
    "print('\\nBest combination:')\n",
    "print(json.dumps(grid_search_results['best_combination'], indent=2))\n",
    "print('\\nBest validation metrics:')\n",
    "print(json.dumps(grid_search_results['best_val_metrics'], indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Best Model\n",
    "\n",
    "Train the model with the best hyperparameters found during grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Found existing best model at results/best_model/best_model.pth, loading...\n",
      "Class names set: {0: 'Normal', 1: 'Abnormal'}\n"
     ]
    }
   ],
   "source": [
    "# Get best parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "grid_search_res_path = 'results/grid_search/grid_search_summary.json'\n",
    "with open(grid_search_res_path, 'r') as f:\n",
    "    grid_search_results = json.load(f)\n",
    "\n",
    "best_params = grid_search_results['best_combination']\n",
    "\n",
    "# Create model with best parameters based on data type\n",
    "if config['data_type'] == 'image':\n",
    "    model = MedicalCNN(\n",
    "        task_type=config['task_type'],\n",
    "        num_classes=config['num_classes'],\n",
    "        num_conv_layers=best_params['num_conv_layers'],\n",
    "        conv_channels=best_params['conv_channels'],\n",
    "        fc_layers=best_params['fc_layers']\n",
    "    )\n",
    "elif config['data_type'] == 'ECG':\n",
    "    model = ECG1DCNN(\n",
    "        task_type=config['task_type'],\n",
    "        num_classes=config['num_classes'],\n",
    "        input_length=config['ecg_max_length'],\n",
    "        num_conv_layers=best_params['num_conv_layers'],\n",
    "        conv_channels=best_params['conv_channels'],\n",
    "        fc_layers=best_params['fc_layers']\n",
    "    )\n",
    "elif config['data_type'] == 'voice':\n",
    "    # Create voice model with best parameters\n",
    "    model = Voice1DCNN(\n",
    "        task_type=config['task_type'],\n",
    "        num_classes=config['num_classes'],\n",
    "        input_length=config['voice_max_length'],\n",
    "        num_conv_layers=best_params['num_conv_layers'],\n",
    "        conv_channels=best_params['conv_channels'],\n",
    "        fc_layers=best_params['fc_layers']\n",
    "    )\n",
    "elif config['data_type'] == 'tabular':\n",
    "    # Create ASD tabular model with best parameters\n",
    "    model = ASDTabularModel(\n",
    "        task_type=config['task_type'],\n",
    "        input_dim=feature_dim,\n",
    "        num_classes=config['num_classes'],\n",
    "        hidden_layers=best_params['hidden_layers'],\n",
    "    )\n",
    "\n",
    "# Define loss function and optimizer\n",
    "if config['task_type'] == 'classification':\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "else:\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = ModelTrainer(\n",
    "    model=model,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    task_type=config['task_type']\n",
    ")\n",
    "\n",
    "# Check if best model exists, if so, load it; otherwise, train and save\n",
    "from pathlib import Path\n",
    "best_model_path = Path(config['save_dir']) / 'best_model' / 'best_model.pth'\n",
    "if best_model_path.exists():\n",
    "    print(f\"Found existing best model at {best_model_path}, loading...\")\n",
    "    trainer.load_model(str(best_model_path))\n",
    "    history = None  # No new training history\n",
    "else:\n",
    "    print(\"No existing best model found, training...\")\n",
    "    history = trainer.train(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=config['num_epochs'],\n",
    "        save_dir=Path(config['save_dir']) / 'best_model',\n",
    "        early_stopping_patience=15\n",
    "    )\n",
    "    # Plot training history\n",
    "    trainer.plot_training_history(Path(config['save_dir']) / 'best_model')\n",
    "\n",
    "# Set class names for classification tasks\n",
    "if config['task_type'] == 'classification':\n",
    "    # Define class name mapping\n",
    "    class_names = class_nms # Modify according to your specific classes\n",
    "    # Or you can set it according to your needs, for example:\n",
    "    # class_names = {0: 'aa', 1: 'bb'}\n",
    "    \n",
    "    # Set class names to trainer\n",
    "    trainer.set_class_names(class_names)\n",
    "    print(f\"Class names set: {class_names}\")\n",
    "\n",
    "# Save the trainer state for later use\n",
    "best_trainer = trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluate the best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████| 4/4 [00:22<00:00,  5.63s/it, Samples=120/120]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Metrics:\n",
      "{\n",
      "  \"accuracy\": 0.875,\n",
      "  \"precision\": 0.8737211064797272,\n",
      "  \"recall\": 0.875,\n",
      "  \"f1\": 0.874285981833505,\n",
      "  \"aupr\": 0.6229885057471264,\n",
      "  \"auc\": 0.8277777777777778\n",
      "}\n",
      "\n",
      "Model device: cpu\n",
      "Task type: classification\n",
      "Test set size: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "test_metrics = trainer.evaluate(test_loader, Path(config['save_dir']) / 'best_model')\n",
    "print('\\nTest Set Metrics:')\n",
    "print(json.dumps(test_metrics, indent=2))\n",
    "\n",
    "# Print some debug information\n",
    "print(f\"\\nModel device: {next(trainer.model.parameters()).device}\")\n",
    "print(f\"Task type: {trainer.task_type}\")\n",
    "print(f\"Test set size: {len(test_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING MODEL COMPARISON WITH TRADITIONAL ML METHODS\n",
      "============================================================\n",
      "Configuration:\n",
      "  Task type: classification\n",
      "  Train set size: 560\n",
      "  Validation set size: 120\n",
      "  Test set size: 120\n",
      "  Class names: {0: 'Normal', 1: 'Abnormal'}\n",
      "  Debug mode: False\n",
      "  Save directory: ./results/model_evaluation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model comparison results saved to ./results/model_evaluation\n",
      "\n",
      "==================================================\n",
      "MODEL PERFORMANCE COMPARISON RESULTS\n",
      "==================================================\n",
      "\n",
      "Deep Learning (CNN):\n",
      "  ACCURACY: 0.8833\n",
      "  PRECISION: 0.8833\n",
      "  RECALL: 0.8833\n",
      "  F1: 0.8833\n",
      "  AUPR: 0.6922\n",
      "  AUC: 0.9148\n",
      "\n",
      "Random Forest:\n",
      "  ACCURACY: 0.8583\n",
      "  PRECISION: 0.8539\n",
      "  RECALL: 0.8583\n",
      "  F1: 0.8492\n",
      "  AUPR: 0.7876\n",
      "  AUC: 0.9319\n",
      "\n",
      "SVM:\n",
      "  ACCURACY: 0.8917\n",
      "  PRECISION: 0.8892\n",
      "  RECALL: 0.8917\n",
      "  F1: 0.8897\n",
      "  AUPR: 0.7409\n",
      "  AUC: 0.9078\n",
      "\n",
      "Logistic Regression:\n",
      "  ACCURACY: 0.8667\n",
      "  PRECISION: 0.8625\n",
      "  RECALL: 0.8667\n",
      "  F1: 0.8634\n",
      "  AUPR: 0.7576\n",
      "  AUC: 0.9300\n",
      "\n",
      "All comparison plots saved to: ./results/model_evaluation/\n",
      "Generated files:\n",
      "- aupr_comparison.png (AUPR curves)\n",
      "- auc_comparison.png (ROC curves)\n",
      "- accuracy_comparison.png\n",
      "- precision_comparison.png\n",
      "- recall_comparison.png\n",
      "- f1_comparison.png\n",
      "- model_comparison_results.json\n"
     ]
    }
   ],
   "source": [
    "# Performance comparison with traditional machine learning methods\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING MODEL COMPARISON WITH TRADITIONAL ML METHODS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Import required modules for comparison\n",
    "\n",
    "from model import compare_models_performance\n",
    "\n",
    "# DEBUG CONTROL - Set to True for detailed debugging information\n",
    "DEBUG_MODE = False  # Change to True if you want to see detailed debug information\n",
    "\n",
    "# Set class names for comparison (if classification task)\n",
    "comparison_class_names = None\n",
    "if config['task_type'] == 'classification':\n",
    "    comparison_class_names = class_names  # Modify according to your classes\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Task type: {config['task_type']}\")\n",
    "print(f\"  Train set size: {len(train_loader.dataset)}\")\n",
    "print(f\"  Validation set size: {len(val_loader.dataset)}\")\n",
    "print(f\"  Test set size: {len(test_loader.dataset)}\")\n",
    "print(f\"  Class names: {comparison_class_names}\")\n",
    "print(f\"  Debug mode: {DEBUG_MODE}\")\n",
    "print(f\"  Save directory: ./results/model_evaluation\")\n",
    "\n",
    "try:\n",
    "    # Compare models performance\n",
    "    comparison_results = compare_models_performance(\n",
    "        best_cnn_trainer=trainer,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader, \n",
    "        test_loader=test_loader,\n",
    "        save_dir='./results/model_evaluation',\n",
    "        task_type=config['task_type'],\n",
    "        class_names=comparison_class_names,\n",
    "        debug=DEBUG_MODE  # Control debug output\n",
    "    )\n",
    "\n",
    "    # Display comparison results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"MODEL PERFORMANCE COMPARISON RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for model_name, metrics in comparison_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"  {metric_name.upper()}: {value:.4f}\")\n",
    "\n",
    "    print(f\"\\nAll comparison plots saved to: ./results/model_evaluation/\")\n",
    "    print(\"Generated files:\")\n",
    "    if config['task_type'] == 'classification':\n",
    "        print(\"- aupr_comparison.png (AUPR curves)\")\n",
    "        print(\"- auc_comparison.png (ROC curves)\")\n",
    "        print(\"- accuracy_comparison.png\")\n",
    "        print(\"- precision_comparison.png\") \n",
    "        print(\"- recall_comparison.png\")\n",
    "        print(\"- f1_comparison.png\")\n",
    "    else:\n",
    "        print(\"- mse_comparison.png\")\n",
    "        print(\"- mae_comparison.png\")\n",
    "        print(\"- r2_comparison.png\")\n",
    "    print(\"- model_comparison_results.json\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during model comparison: {e}\")\n",
    "    if DEBUG_MODE:\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teacher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
